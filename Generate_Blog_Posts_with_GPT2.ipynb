{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajasafi/Generate-Blog-Posts-with-GPT2/blob/main/Generate_Blog_Posts_with_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLLOhPUMRPXg"
      },
      "source": [
        "# 1. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXMnJxIZbDvC"
      },
      "source": [
        "**we** need to do is install and import our dependencies then what we need to do is load our hugging face gpt2 model then what we're going to do is tokenize that sentence so what this means is reduce it to individual words and then effectively encode them which means convert them to a number representation then what we're going to do is generate our text and decode it so we'll convert it from a number to its representative word and then we'll output our result so in order to actually do this our core dependency is actually going to be hugging face transformers and specifically we're going to be using the gpt2 **model** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6K3HRUmQQH4",
        "outputId": "32632a1b-ae41-4143-d738-acb0d36084f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.6 MB 24.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 106 kB 63.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 278 kB 70.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 49.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 856 kB 43.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 593 kB 57.2 MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 29.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 43.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LRMZI-4RH_g"
      },
      "source": [
        "Tokenizer is going to take our input text and encode it from a word to an effective number what we're then going to do is pass it to the model which is our gpt2 model and this is going to generate output tokens so it's really going to generate numbers and these numbers represent words or word sequences we'll then use this tokenizer again to decode those numbers back to word so effectively we're getting words converting it to numbers passing it to the model model is going to generate new number sequences we'll then pass it back to our tokenizer to decode it and we're going to get an output sequence which effectively represents a blog post now the next thing that we need to do is actually go on ahead and load our model and our tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYf1xtaSZLTN"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5gVtzQteaG3"
      },
      "source": [
        "\n",
        "\n",
        "1. So that's our tokenizer and our model loaded so we've written two lines of code there so first up what we've done is we've created our tokenizer and in order to do that we've written a new variable or created a new variable called tokenizer and then we've set it equal to gpt2 tokenizer and then we've used the from pre-trained method to load up our pre-trained gpt to large tokenizer so in order to do that if we read through the whole line so it's tokenizer equals gpt2 tokenizer dot from pre-trained and then we've passed through a parameter which is gpt2 dash large so this effectively allows us to leverage the gpt2 large model then we've also instantiated a model.\n",
        "2.   specifically what we've gone and done is we've used our gpt2 lm head model which is what we imported up here and again we've used the from pre-trained model to load up our pre-trained model and again we've imported the gpt to large model now if you get a out-of-memory error or if you get something along those lines you may want to use the non-large model in which case you can just remove the dash large and this will allow you to load the the smaller version of the model but in this case we're going to use the large model because it allows us to work and generate bigger more sophisticated blocks of text all right then the next keyword parameter that we've passed through is the pad token id.\n",
        "  \n",
        "\n",
        "* So if we take a look at that whole line that we've written there so we've basically said model equals gpt to lm head model dot from pre-trained and then we've passed through gpt to large as the model that we want to load up and then we've specified pad underscore token underscore id equals tokenizer dot eos token id.\n",
        "\n",
        "* **This basically represents end of sentence token id so that's the identifier for what token represents an end of a sentence in this case it was that number that you saw and if we decode that the word that's actually represented there is a set of brackets uh some bars and then end of text and then the same on the other side**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkPR5XbVg-2r"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2jGZOwIkeLQ"
      },
      "source": [
        "# 2. Tokenize Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6Ybim-P1jKS5"
      },
      "outputs": [],
      "source": [
        "sentence = 'software engineering'\n",
        "input_ids = tokenizer.encode(sentence, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsYDlvJ8jSbw",
        "outputId": "d69cbcf8-8800-4cd6-b863-d6d9ba0f8a2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[43776,  8705]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "input_ids "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=500, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)"
      ],
      "metadata": {
        "id": "t3GFpxZMACk7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka9DEW9cARmj",
        "outputId": "6943e264-c9d7-45ff-f8ab-5a14c406ff3c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[43776,  8705,     1,   290,   366, 43776,  2478,  1911,   198,   198,\n",
              "             1, 25423,  2478,     1,   318,  5447,   355,   366,  1169,  1429,\n",
              "           286, 18492,    11,  5922,    11,  4856,    11,   290, 29682,   257,\n",
              "          3644,  1430,   393,  3644,  1080,    11,  1390,    11,   475,   407,\n",
              "          3614,   284,    11,   262,  1486,   290,  2478,   286,  3644,  4056,\n",
              "           290,  3644,  3341,   329,   262,  4007,   286, 10068,   262,  2854,\n",
              "            11, 17843,    11, 10152,   286,   779,    11,   393,   584,  9695,\n",
              "           286,   884,  4056,   393,  3341,    11,   355,   880,   355,   262,\n",
              "          2478,   290,  4856,   286,  3788,  5479,   329,   779,   287,   884,\n",
              "          9061,   290,  3341,    13, 10442,  2478,   743,   635,  2291,   262,\n",
              "         11824,   286, 10314,    11,   884,   355, 10314,   326,  8477,   703,\n",
              "           257,  1430,   318,   284,   307,   973,    11,   703,   284,   779,\n",
              "           340,    11,   644,   284,   466,   611,   340,   318,   407,  1762,\n",
              "           355,  2938,   393,   611,   612,   389,  2761,   351,   262,  1430,\n",
              "            13, 43925,   743,   307,  3194,   287,   257,  4996,   286, 17519,\n",
              "           290,   743,  2291,    11,   329,  1672,    11, 41371,    11,  6276,\n",
              "          3136,    11,  2836,   338, 17555,    11,  1332,  4056,    11,  2723,\n",
              "          2438,    11,  3503,   526,   357,  4023,  1378,   268,    13, 31266,\n",
              "            13,  2398,    14, 15466,    14, 25423,    62, 31267,    62,     4,\n",
              "          2078, 33215,    62, 23065,  2229,     4,  1959,     2, 24941,   341,\n",
              "            62,   259,    62,  1169,    62, 17013,    62, 42237,    62,   392,\n",
              "            62,   847,    62,  9127,  1678,    62,  1659,     4,    36,    17,\n",
              "             4,  1795,     4,    24,    35,    62,  1525,    62, 23213,   563,\n",
              "           737,   628,   198, 25423,  8705, 10229,   284,   262,  1429,   416,\n",
              "           543,  3788,   318,  3562,    11,  4166,    11,  6789,    11, 14257,\n",
              "          2004,   290, 12380,    13,   632,   318,   257,  3154,  3381,   326,\n",
              "          8698,   257,  3094,  2837,   286,  4568,    11,   422,   262,  6282,\n",
              "           286,   262,  3788,   284,   663,  4856,   290, 14833,    13,   383,\n",
              "          3381,  3788,  8705,   318,  1690,   973, 26478,  1346,   351,  3788,\n",
              "          2478,    11,  3584,   262,   734,  2846,   389,   407, 26519,  8568,\n",
              "            13,  1114,   517,  1321,    11,   766,   262, 15312,  2708,   319,\n",
              "         10442, 14044,   290,   262,  1294,  2732,   286, 16127,   338,  9840,\n",
              "           286, 17420,   290,  4765,   338,   357,    33,  1797,     8,  6188,\n",
              "          4765,   290, 19602,  8549,   357,  1797, 29138,     8,  3052,   357,\n",
              "          5450,  1378,  2503,    13, 41907,    13,  9567,    14,   271, 26224,\n",
              "            14,   737, 50256]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(output[0], skip_special_tokens=True , clean_up_tokenization_spaces=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "9N457IPuAkRn",
        "outputId": "dc1c12dc-6847-45db-9056-5727ee184bbc"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'software engineering\" and \"software development\".\\n\\n\"Software development\" is defined as \"the process of designing, developing, testing, and deploying a computer program or computer system, including, but not limited to, the design and development of computer programs and computer systems for the purpose of improving the performance, reliability, ease of use, or other characteristics of such programs or systems, as well as the development and testing of software applications for use in such computers and systems. Software development may also include the preparation of documentation, such as documentation that describes how a program is to be used, how to use it, what to do if it is not working as expected or if there are problems with the program. Documentation may be written in a variety of formats and may include, for example, manuals, technical reports, user\\'s guides, test programs, source code, etc.\" (http://en.wikipedia.org/wiki/Software_development_%28computer_programming%29#Documentation_in_the_United_States_and_other_countries_of%E2%80%9D_by_industry).\\n\\n\\nSoftware engineering refers to the process by which software is designed, developed, tested, debugged and deployed. It is a broad term that covers a wide range of activities, from the creation of the software to its testing and deployment. The term software engineering is often used interchangeably with software development, although the two terms are not mutually exclusive. For more information, see the Wikipedia article on Software Engineering and the US Department of Commerce\\'s Bureau of Industry and Security\\'s (BIS) Information Security and Risk Management (ISRM) website (https://www.bis.gov/isrm/).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.decode(output[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "lLDpmj2rTEqV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('SE.txt', 'w') as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "th-495DLTG8C"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URJ4_9-rkk2D"
      },
      "source": [
        "# 3.  Generate And Decode Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "EI_xpkKjjTEQ"
      },
      "outputs": [],
      "source": [
        "# def generate_blog(INPUT):\n",
        "#     input_ids = tokenizer.encode(INPUT, return_tensors='tf')\n",
        "#     output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "#     text = tokenizer.decode(output[0], skip_special_tokens=True , clean_up_tokenization_spaces=True)\n",
        "#     return \".\".join(text.split(\".\")[:-1])+ \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "uKhXn-n5jWqr",
        "outputId": "7386fef8-e520-4173-bed9-6071918fab4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7872, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# output_text = gr.outputs.Textbox()\n",
        "# gr.Interface(generate_blog,\"textbox\",output_text,title=\"GPT-2\", description=\" Open AI GPT-2 is a unsupervised language model that\\ can generate coherent text INPUT a sentence and see what it completes with its.\\it will take 20second to run. \").launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "yQdXcZmFk0_-",
        "outputId": "9af58409-5f4b-4d79-d04b-eb8d55e7a35a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What is WordCounter?\\n\\nWordCounter is a free, open-source, cross-platform word-counting application for Windows, Mac OS X, and Linux. It can be used to count the number of words in any text file, including HTML, PDF, Word documents, spreadsheets, presentations, or any other text-based document format. The application is designed to be easy to use, yet powerful enough to handle large numbers of text files. For more information, visit www.wordcounter.com.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tmrZ4_oktM-"
      },
      "source": [
        "# 4. output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqNFiGnZnKqZ"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ybLWBXmkTAaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ivos3ysGne63"
      },
      "outputs": [],
      "source": [
        "with open('petrol_price_hike.txt', 'w') as f:\n",
        "  f.write(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNwVXT2zWvEKkjn74IQ8vO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}